@misc{TDMPCv1,
      title={Temporal Difference Learning for Model Predictive Control},
      author={Nicklas Hansen and Xiaolong Wang and Hao Su},
      year={2022},
      eprint={2203.04955},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.04955},
}

@misc{TDMPCv2,
      title={TD-MPC2: Scalable, Robust World Models for Continuous Control},
      author={Nicklas Hansen and Hao Su and Xiaolong Wang},
      year={2024},
      eprint={2310.16828},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.16828},
}

@misc{DreamerV3,
      title={Mastering Diverse Domains through World Models},
      author={Danijar Hafner and Jurgis Pasukonis and Jimmy Ba and Timothy Lillicrap},
      year={2024},
      eprint={2301.04104},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2301.04104},
}

@misc{ELU,
      title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
      author={Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
      year={2016},
      eprint={1511.07289},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.07289},
}

@misc{Mish,
      title={Mish: A Self Regularized Non-Monotonic Activation Function},
      author={Diganta Misra},
      year={2020},
      eprint={1908.08681},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.08681},
}

@misc{iCEM,
      title={Sample-efficient Cross-Entropy Method for Real-time Planning},
      author={Cristina Pinneri and Shambhuraj Sawant and Sebastian Blaes and Jan Achterhold and Joerg Stueckler and Michal Rolinek and Georg Martius},
      year={2020},
      eprint={2008.06389},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2008.06389},
}

@InProceedings{HaarnojaAbbeelLevine2018:SAC,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author = 	 {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {http://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}

@InProceedings{fujimoto2018:TD3,
  title = {Addressing Function Approximation Error in Actor-Critic Methods},
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages = {1587--1596}, year = {2018}, editor = {Jennifer Dy and Andreas Krause},
  volume = {80}, series = {Proceedings of Machine Learning Research},
  address = {Stockholmsmässan, Stockholm Sweden},
  month = {10--15 Jul},
  publisher = {PMLR},
  pdf = {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = {http://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}

@InProceedings{eberhard2023pink,
  title={Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning},
  author={Eberhard, Onno and Hollenstein, Jakob and Pinneri, Cristina and Martius, Georg},
  journal={International Conference on Learning Representations},
  year={2023}
}
@inproceedings{SchaulPER2016,
  author       = {Tom Schaul and
                  John Quan and
                  Ioannis Antonoglou and
                  David Silver},
  title        = {Prioritized Experience Replay},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.05952},
  timestamp    = {Wed, 24 Sep 2025 16:21:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchaulQAS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@techreport{haarnoja2018sacapps,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in {S}tar{C}raft {II} using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@misc{vieillard2020munchausenreinforcementlearning,
  title={Munchausen Reinforcement Learning},
  author={Nino Vieillard and Olivier Pietquin and Matthieu Geist},
  booktitle={Advances in Neural Information Processing Systems},
  editor={H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  volume={33},
  pages={8059--8069},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf}
}

@misc{burda2018explorationrandomnetworkdistillation,
  title={Exploration by Random Network Distillation},
  author={Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
  year={2018},
  eprint={1810.12894},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1810.12894},
}

@misc{bartolGithubRepo,
  author = {Bhatia, Rajinish Aneel and Markovinović, Bartol and Siddiqui, Mohd Khizir},
  title = {Hockeyrl},
  year = {2026},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/BartolMarko/HockeyRL}},
}
