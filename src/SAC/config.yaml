dry_run: False

# Environment
n_actions: 4
opponents:
  - type: 'StrongBot'
    priority: 0.6
  - type: 'WeakBot'
    priority: 0.4
# rewards
reward_transform: "v0"
# rewards: v1
reward_scale: 2.1
closeness_to_puck_reward_weight: 1.2
puck_direction_reward_weight: 1.2
touch_puck_reward_weight: 1.2
reward_step_penalty: 0.02
# rewards: v3 / v4
draw_penalty: 5 # also reward v1
still_puck_penalty: 0.25

# Buffers
buffer_type: "n-step-per"
buffer_max_size: 1000000
# buffer_type: n-step-per
n_step_buffer_n: 4

# Training
n_games: 50000
warmup_games: 200
learn_steps_per_episode: 1
batch_size: 2048
lr_actor: 0.0004
lr_critic: 0.0003
gamma: 0.99
tau: 0.005
# alpha
automatic_entropy_tuning: True
lr_alpha: 0.0005
# critic target network
target_update_freq: 1
# Explorer
explorer:
  type: random

  # type: 'gaussian'
  # mu: 0.5
  # std_dev: 0.85

  # type: ou
  # theta: 0.25
  # sigma: 0.3

  # type: optimistic
  # beta: 1.1

  # type: curious
  # beta: 0.3
  # hidden_dim: 64
  # lr_pred: 0.001

# Network
algorithm: "sac"
hidden_size: 1024

# Evaluation
eval_freq: 400
eval_episodes: 20

# Model saving
save_model: True
save_model_freq: 400

# Logging
use_wandb: True
wandb_project: "hockey-rl"
exp_name: "v0-random-4-step-per-6_4-bot-pool"
resume: False
save_video: True
log_gradients: False
