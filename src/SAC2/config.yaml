dry_run: False

# Environment
n_actions: 4
weak_opponent: True

# Buffers
buffer_max_size: 1000000

# Training
n_games: 20000
warmup_games: 50
learn_steps_per_episode: 2
batch_size: 256
lr_actor: 0.0003
lr_critic: 0.0003
gamma: 0.99
tau: 0.005
# alpha
automatic_entropy_tuning: True
lr_alpha: 0.001
# critic target network
target_update_freq: 1
# rewards
reward_transform: "v1"
reward_scale: 2.1
closeness_to_puck_reward_weight: 1.2
puck_direction_reward_weight: 1.2
touch_puck_reward_weight: 1.2
reward_step_penalty: 0.02

# Network
hidden_size: 256

# Evaluation
eval_freq: 100
eval_episodes: 20

# Model saving
save_model: True
save_model_freq: 300

# Logging
use_wandb: True
wandb_project: "hockey-rl"
exp_name: "reward-v1-sac"
save_video: True
log_gradients: False
