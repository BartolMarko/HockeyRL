dry_run: True

# Environment
n_actions: 4
opponent_pool:
  type: 'weighted'
opponents:
  - type: 'StrongBot'
    priority: 0.40
  - type: 'WeakBot'
    priority: 0.20
  - type: PuckFollowBot
    priority: 0.20
  - type: 'CustomAgent'
    experiment_name: sac-v0-gaussian-replay-self-play-mirror
    priority: 0.40
self_play:
  - name: 'SelfPlayMgr'
    priority: 0.7
    max_pool_size: 20
    activation_type: 'botwin'
    activation_epsilon: 0.9
    pooling:
      - type: 'episode'
        freq: 80
    sampler:
      name: 'pfsp'
      pfsp_p: 0.5
num_envs: 16

# rewards
reward_transform: "v4"
still_puck_penalty: 0.005
draw_penalty: 4

# Buffers
buffer_type: "n-step-per"
buffer_max_size: 1500000
upside_down: True
n_step_buffer_n: 6

# Training
n_games: 7500
warmup_games: 300
learn_steps_per_episode: 1
batch_size: 1024
lr_actor: 0.0004
lr_critic: 0.0003
gamma: 0.99
tau: 0.005
# alpha
automatic_entropy_tuning: True
lr_alpha: 0.00025
target_entropy: phasic
# critic target network
target_update_freq: 1
# Explorer
explorer:
  type: pink

  # type: random

  # type: 'gaussian'
  # mu: 0.5
  # std_dev: 0.85

  # type: ou
  # theta: 0.25
  # sigma: 0.3

  # type: optimistic
  # beta: 1.1

  # type: curious
  # beta: 0.3
  # hidden_dim: 64
  # lr_pred: 0.001
# Munchausen RL
# comment: though MRL says PER and N-Step-Returns is not needed
# but we still do it, should check without it too
use_munchauser: True
munchausen_alpha: 0.9
munchausen_lo_clip: -1

# Network
algorithm: "sac"
hidden_size: 512

# Evaluation
eval_freq: 239
eval_episodes: 32

# Model saving
save_model: True
save_model_freq: 400

# Logging
use_wandb: True
wandb_project: "hockey-rl"
exp_name: "sac-v4-pink-6-step-per-old-agents-MRL-4"
resume: False
save_video: True
log_gradients: True
