dry_run: False

# Environment
n_actions: 4
opponent_pool:
  type: 'weighted'
opponents:
  - type: 'StrongBot'
    priority: 0.65
  - type: 'WeakBot'
    priority: 0.35
self_play:
  - name: 'SelfPlayMgr'
    priority: 0.8
    max_pool_size: 1000
    activation_type: 'botwin'
    activation_epsilon: 1.0
    pooling:
      - type: 'episode'
        freq: 5
    sampler:
      name: 'pfsp'
      pfsp_p: 0.5
num_envs: 16

# rewards
reward_transform: "v0"
# rewards: v1
reward_scale: 2.1
closeness_to_puck_reward_weight: 1.2
puck_direction_reward_weight: 1.2
touch_puck_reward_weight: 1.2
reward_step_penalty: 0.02
# rewards: v3 / v4
draw_penalty: 5 # also reward v1
still_puck_penalty: 0.25

# Buffers
buffer_type: "replay"
buffer_max_size: 1000000

# Training
n_games: 75000
warmup_games: 200
learn_steps_per_episode: 1
batch_size: 1024
lr_actor: 0.0004
lr_critic: 0.0003
gamma: 0.99
tau: 0.005
# alpha
automatic_entropy_tuning: True
lr_alpha: 0.0005
# critic target network
target_update_freq: 1
# Explorer
explorer:
  type: random

  # type: 'gaussian'
  # mu: 0.5
  # std_dev: 0.85

  # type: ou
  # theta: 0.25
  # sigma: 0.3

  # type: optimistic
  # beta: 1.1

  # type: curious
  # beta: 0.3
  # hidden_dim: 64
  # lr_pred: 0.001

# Network
algorithm: "sac"
hidden_size: 512

# Evaluation
eval_freq: 400
eval_episodes: 20

# Model saving
save_model: True
save_model_freq: 400

# Logging
use_wandb: True
wandb_project: "hockey-rl"
exp_name: "sac-v0-random-replay-self-play-puffer"
resume: False
save_video: True
log_gradients: False
