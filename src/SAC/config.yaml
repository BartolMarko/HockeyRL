dry_run: True

# Environment
n_actions: 4
opponent_pool:
  type: 'weighted'
opponents:
  - type: StrongBot
    priority: 0.15
  - type: WeakBot
    priority: 0.1
  - type: PuckFollowBot
    priority: 0.15
  - type: CustomAgent
    experiment_name: sac-v4-pink-4-step-per
    priority: 0.2
  - type: CustomAgent
    experiment_name: sac-v0-gaussian-replay-self-play-mirror
    priority: 0.2
  - type: CustomAgent
    experiment_name: sac-v4-pink-4-step-per-league-env-reset
    priority: 0.2
  - type: CustomAgent
    experiment_name: sac-v4-pink-4-step-per-league-env-defence
    priority: 0.15
  - type: CustomAgent
    experiment_name: sac-v4-pink-6-step-per-env-defence
    priority: 0.15
self_play:
  - name: SelfPlayMgr
    priority: 0.5
    max_pool_size: 10
    activation_type: botwin
    activation_epsilon: 0.85
    pooling:
    - type: episode
      freq: 120
    sampler:
      name: pfsp
      pfsp_p: 0.6
num_envs: 16
env_left_start_modes:
  - [ 'normal', 2500 ]
  - [ 'custom', 0.2, 4000 ]
  - [ 'normal', 6000 ]
  - [ 'custom', 0.8, 8000 ]
  - [ 'normal' , 9000 ]
  - [ 'custom', 0.3, 10000 ]
  - [ 'normal' , 11000 ]
# rewards
reward_transform: "v5"
still_puck_penalty: 0.005
draw_penalty: 5

# Buffers
buffer_type: "n-step-per"
buffer_max_size: 1000000
upside_down: True
n_step_buffer_n: 6

# Training
n_games: 13000
warmup_games: 300
learn_steps_per_episode: 1
batch_size: 1024
lr_actor: 0.0004
lr_critic: 0.0003
gamma: 0.99
tau: 0.005
# alpha
automatic_entropy_tuning: True
lr_alpha: 0.00025
# critic target network
target_update_freq: 1
# Explorer
explorer:
  type: pink
# Munchausen RL
use_munchausen: False

# Network
algorithm: "sac"
hidden_size: 1024

# Evaluation
eval_freq: 250
eval_episodes: 32

# Model saving
save_model: True
save_model_freq: 300

# Logging
use_wandb: True
wandb_project: "hockey-rl"
exp_name: "sac-v5"
resume: False
save_video: True
log_gradients: True
