run_name: tdmpc_self_play_150k_start_100k_freq_2M_steps

# environment
env_name: SparseRewardHockeyEnv
action_repeat: 1
discount: 0.99
max_episode_length: 251
train_steps: 2000000

# planning
iterations: 6
num_samples: 512
num_elites: 64
mixture_coef: 0.05
min_std: 0.05
temperature: 0.5
momentum: 0.1

# learning
batch_size: 512
max_buffer_size: 1000000
horizon: 5
reward_coef: 0.5
value_coef: 0.1
consistency_coef: 2
rho: 0.5
kappa: 0.1
lr: 1e-3
std_schedule: linear(0.5, 0.05, 25000)
horizon_schedule: linear(1, 5, 25000)
per_alpha: 0.6
per_beta: 0.4
grad_clip_norm: 10
seed_steps: 5000
update_freq: 2
tau: 0.01

# architecture
enc_dim: 256
mlp_dim: 512
latent_dim: 50

# misc
seed: 1
exp_name: default
eval_freq: 20000
save_video: false
save_model: True
save_model_freq: 50000
device: cuda

# Opponent Pool
opponent_pool_window_size: 100
opponent_pool_draw_weight: 0.5

# Evaluation
eval_episodes_per_opponent: 50
final_eval_episodes_per_opponent: 100
video_episodes_per_outcome: 30

# Additional opponents for evaluation
evaluation_opponents:
  TD3_119k_faker:
    type: TD3
    weights_path: ./models/td3_benchmarks/td3_HockeyEnv_119.pt
    config_path: ./models/td3_benchmarks/td3_config.yaml
  TD3_118k_bank_shot:
    type: TD3
    weights_path: ./models/td3_benchmarks/td3_HockeyEnv_118.pth
    config_path: ./models/td3_benchmarks/td3_config.yaml

# Training opponents
training_opponents:
  StrongBot:
    type: StrongBot
    start_step: 0
  WeakBot:
    type: WeakBot
    start_step: 0

# Self play
selfplay: true
selfplay_start_step: 150000
selfplay_freq: 100000